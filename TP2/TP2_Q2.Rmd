---
title: "Travail pratique 2"
subtitle: "<span style='font-size: 35px'>Question 2 - Commercialisation et prévisibilité du succès de produits</style>" 
author:
- Équipe 17
- Loïc Artino (536 756 361)
- François L'Écuyer (901 346 493)
date: "28 octobre 2020"
output:
  html_document:
    code_folding: "hide"
    highlight: haddock
    output: html_document
    theme: default
    toc : true
    toc_float: true
    toc_depth: 2
---

```{r librairies, include=FALSE}

# Chargement des librairies

packages<-function(x){
  x<-as.character(match.call()[[2]])
  if (!require(x,character.only=TRUE)){
    install.packages(pkgs=x,repos="http://cran.r-project.org")
    require(x,character.only=TRUE)
  }
}
packages(tidyverse)
packages(rmarkdown)
packages(stringr)
packages(viridis)
packages(caret)
packages(rpart.plot)
packages(pROC)

# stringr est un outil de manipulation des chaînes de caractères
# viridis est un outil de gestion des couleurs dans les visuels de type ggplot
# caret est un outil qui contient les fonctions pour simplifier le processus de création des régressions et des classifications 
# rpart.plot est un complément de la fonction prp
# pROC est un outil permettant de générer les courbes ROC

```

# Question 2.1 - Préparer les données

## Chargement des données

La première étape de la préparation des données consiste à charger les données à partir d'un fichier TSV (tabulated-separated value).

```{r load_data, message=FALSE, results='hide', comment=NA, warning=FALSE, echo=TRUE}

# Chargement des données séparées par des tabulations

data_q2 = read_tsv("data/Q2_data.tsv")

```

Le chargement initial des données a permis de recevoir **`r nrow(data_q2)`** observations pour des vins différents dont on peut voir les cinq premières observations dans le Tableau 1.
`r knitr::kable(head(data_q2, 5), caption = "**Tableau 1 - Liste partielle des données**")`

## Nettoyage des données

```{r missing, message=FALSE, results='hide', comment=NA, warning=FALSE, echo=TRUE}

# Vérification des erreurs

data_q2 %>% summarize(na_count = sum(is.na(.)))

# Vérification des types de données

str(data_q2)

# Transformation de la colonne Class d'un format Caractère à un format Catégoriel   

data_q2$Class <- as.factor(data_q2$Class)

# Changement du nom des colonnes

data_q2 = data_q2 %>%
  rename(couleur = in01, 
          tanin = in02, 
          densite_420nm = in03,
          densite_520nm = in04,
          densite_620nm = in05,
          acidite_fixe = in06,
          acidite_volatile = in07,
          acidite_citr = in08,
          sucre = in09,
          chlorures = in10,
          SO2_libre = in11,
          dens_SO2_tot = in12,
          densite = in13,
          pH = in14,
          sulfates = in15,
          teneur_alcool = in16,
          prix_moy_cad = in17)
```
La deuxième étape de la préparation consiste à faire le nettoyage de nos données pour les rendre conformes aux besoins d'analyse subséquent.Cela s'est fait en vérifiant s'il n'y a pas de valeurs manquantes, si les types des colonnes sont appropriés et si toutes les colonnes sont nécessaires.Suite à la vérification du nombre d'erreurs à partir de la commande **"na_count()**, nous avons pu constater qu'il y avait **`r sum(is.na(data_q2))`** donnée(s) manquante(s) dans le fichier importé donc nous n'avons pas eu à faire d'opération pour corriger cela. Par ailleurs, nous avons vérifier les types des colonnes à partir de la commande **str()**. Cela nous a permis de constater que 4 colonnes étaient de type Chaine de caractères dont la colonne de sortie (Class). Nous avons donc décidé de changer le type de la colonne Class pour un type Catégoriel (fct) selon ce qui est requis pour le partitionnement des données et les analyses subséquentes. Nous avons aussi décidé de conserver l'ensemble des colonnes. On peut voir au Tableau 2, les cinq premières observations et quelques colonnes de la table modifiée.
`r knitr::kable(head(data_q2, 5), caption = "**Tableau 2 - Liste partielle des données préparées**")`

## Partitionnement les données

```{r modele_1part, message=FALSE, results='hide', comment=NA, warning=FALSE, echo=TRUE}

# Fixation de la valeur pour la sélection pseudo-aléatoire

set.seed(536)

# Partitionnement des données en des ensembles d'entrainement et de test

train_q2_indices = createDataPartition(y = data_q2$Class,
                                       p = 0.70,
                                       list = FALSE,
                                       times = 1)

train_q2 = data_q2 %>% slice(train_q2_indices)
test_q2 = data_q2 %>% slice(-train_q2_indices)

```
La troisième étape consiste à partitionner les observations en deux ensembles de façon pseudo-aléatoire. Un ensemble d'entraînement permettra de construire le meilleur modèle et un ensemble test permettra de vérifier la performance de ce modèle. La commande **set.seed()** a été fixée à **536** de façon arbitraire pour pouvoir partitionner les données de façon aléatoire mais selon un même schème pour pouvoir comparer nos résultats d'une fois à l'autre. Les paramètres de la partition ont été fixés à 0.70 pour le **p** de façon à avoir 70 % de données dans la partition d'entraînement et à 1 pour le paramètres **times** de façon à avoir seulement une partition pour les deux ensembles d'entraînement et de test. Cette création a permis d'obtenir **`r nrow(train_q2)`** observations pour l'ensemble d'entrainement et **`r nrow(test_q2)`** pour l'ensemble test. 

# Question 2.2 - Entraînement des modèles

Nous avons décidé de comparer trois modèles pour la prédiction du succès des vins. Le premier est un arbre de décision (rpart2), le second est un réseau neuronal (nnet) et le troisième est une analyse discriminante linéaire (lda). 

La première étape de l'entraînement a consisté à créer un objet trainControl qui permet d'en spécifier les paramètres. À cet effet, nous avons utilisé la méthode **repeatedcv** qui nous permet de définir le nombre de partitions avec lesquels nous désirons entraîner nos données et le nombre de répétition que nous désirons faire. Dans ce cas, le nombre de parition a été fixée à **10** et le nombre de répétitions à **10** aussi. La deuxième étape a consisté à produire des modèles différents à partir de ce jeu de données pour produire ceux qui convenaient le mieux pour chaque méthodes selon les paramètres qui leur sont propres, et cela, à partir d'une fichier RDS créé lors de la première utilisation de chaque méthode.

## Modèle 1 - Arbre de décision (rpart2)

La méthode rpart2 (Recursive Partitioning And Regression Trees) est un outil créant un arbre de décision en intelligence prédictive.Les arbres de décision permettent d'valuer les différentes alternatives sous la forme graphique d'un arbre où les décisions possibls se situent aux extrémités des branches. 

```{r modele_1, message=FALSE, results='hide', comment=NA, warning=FALSE, echo=TRUE}

#  Création de la structure de données avec la fonction cross-validation). Sera utilisée pour tous les modèles.

dt_fit_control = trainControl(method = "repeatedcv",
                                number = 10,
                                repeats = 10)

# Entrainement du modèle 1 et enregistrement dans un fichier RDS

if (file.exists("rpart2_mod.rds") == TRUE) {
  rpart2_mod = readRDS("rpart2_mod.rds")
} else {
  rpart_tune = expand.grid(maxdepth = 1:25)
  rpart2_mod = train(Class ~ .,
                    data = train_q2,
                    trControl = dt_fit_control,
                    method = "rpart2",
                    tuneGrid = rpart_tune)
  saveRDS(rpart2_mod, file = "rpart2_mod.rds")
}

```

## Modèle 2 - Réseau neuronal (nnet)

Un réseau neuronal est un outil prévisionnel basé sur le fonctionnement du cerveau i.e.les neurones. Dans ce modèle, nous fixons les deux paramètres que sont le nombre de neurones que la couche cachée (size), ainsi que plusieurs pondérations (decay) qui limitent le surapprentissage du modèle.

```{r modele_2, message=FALSE, results='hide', comment=NA, warning=FALSE, echo=TRUE}

# Entrainement du modèle 2 et enregistrement dans un fichier RDS

if (file.exists("nnet_mod.rds") == TRUE) {
  nnet_mod = readRDS("nnet_mod.rds")
} else {
  nnet_tune = expand.grid(decay = c(0, 0.01, .1),
                           size = c(1:5))
  nnet_mod = train(Class ~ .,
                    data = train_q2,
                    trControl = dt_fit_control,
                    method = "nnet",
                    tuneGrid = nnet_tune)
  saveRDS(nnet_mod, file = "nnet_mod.rds")
}

```

## Modèle 3 - Analyse discriminante linéaire (lda)
L'analyse discriminante linéaire ou lda en anglais est une technique d'analyse discriminante prédictive où il est possible d'expliquer ou de prédire                    l'appartenance d'un élément à un groupe à partir de ses caractéristiques mesurées à l'aide de variables prédictives.

```{r modele_3, message=FALSE, results='hide', comment=NA, warning=FALSE, echo=TRUE}

# Entrainement du modèle 3 et enregistrement dans un fichier RDS

if (file.exists("lda_mod.rds") == TRUE) {
  lda_mod = readRDS("lda_mod.rds")
} else {
  lda_mod = train(Class ~ .,
                    data = train_q2,
                    trControl = dt_fit_control,
                    method = "lda")
  saveRDS(lda_mod, file = "lda_mod.rds")
}

```

# Question 2.3 - Afficher l'arbre de décision appris

Vous pouvez visualiser à la figure suivante l'arbre de décision construite à partir du modèle rpart2 et des données d'entraînement.

```{r decision_tree, message=FALSE, results='hide', comment=NA, warning=FALSE, echo=TRUE}

# Affichage de l'arbre de décision

prp(rpart2_mod$finalModel,
    box.palette = "Reds",
    type = 5,
    extra = "auto")

```

# Question 2.4 - Évaluation de l'entraînement des modèles

Chaque modèle a ensuite été évalué en ciblant le modèle le plus performant pour chaque méthode de façon à pouvoir comparer le meilleur de chacun d'eux. Ainsi, il a été possible de comparer l'efficacité de chaque modèle en fonction de ce qui était attendue pour chaque variable dépendante et le résultat réel. La performance de chaque modèle a été évaluée selon des critères qui leur sont propres et qu'il est possible d'évaluer.  

## Modèle 1 - Arbre de décision (rpart2)

On peut voir à la figure suivante que le modèle le plus performant (en rouge) est présent lorsque la profondeur maximale est de 11. Par ailleurs, la matrice de confusion nous permet de voir les éléments du calcul de différentes statistiques comme, entre autres, la précision et le Kappa de Cohen que nous utiliserons plus loin pour comparer les modèles entre eux.

```{r performance_1, message=FALSE, results='hide', comment=NA, warning=FALSE, echo=TRUE}

# Affichage du meilleur modèle 1

rpart2_mod$bestTune
rpart2_mod$finalModel

# On visualise la performance du modèle 1

rpart2_mod$results %>%
  mutate(best_md = ifelse(maxdepth == 11, TRUE, FALSE)) %>%
  ggplot(aes(x = maxdepth,
            y = Accuracy)) +
    geom_ribbon(aes(ymin = Accuracy - AccuracySD,
                    ymax = Accuracy + AccuracySD), alpha = 0.2) +
    geom_point(aes(color = best_md)) +
    geom_line() +
    scale_color_manual(values = c('#595959', 'red')) +
    theme_minimal() +
    ggtitle("Précision du modèle en fonction de \nla profondeur maximale des arbres") +
    theme(plot.title = element_text(family = 'Helvetica', face = 'bold', hjust = 0.5), legend.position = "none") +
    labs(x = "Profondeur maximale", y = "Précision (Accuracy)")

# Phase de test du modèle 1

dt_pred_class1 = predict(rpart2_mod, test_q2)

dt_pred_class1 %>% 
  head(5)

# Matrice de confusion - Modèle 1

dt_conf_mat1 = confusionMatrix(dt_pred_class1,
                               reference = test_q2$Class,
                               positive = "yes",
                               mode = "everything")

```

`r knitr::kable(head(dt_conf_mat1[["table"]], 2), caption = "**Tableau 3 - Matrice de confusion du modèle 1**")`

## Modèle 2 - Réseau neuronal (nnet)

Pour le réseau neuronal, on peut voir à la première figure le nombre des résidus en fonction de chaque valeur. On peut y voir que la distribution est beaucoup centrée sur 0 avec une certaine symétrie à la gauche et à la droite du 0, ce qui donne une bonne approximation de la qualité du modèle utilisé. Les deux autres graphiques permettent aussi d'apprécier la qualité du modèle selon sa prédictivité et les différentes pondérations utilisées (decay).

La valeur nulle sert ici de témoin ; on remarque que la précision qui découle de l'entraînement du modèle est moindre, ce qui est sûrement dû au surapprentissage et cela vient conforter l'utilisation d'une pondération adéquate. Dans notre cas, une pondération de 0.1 semble être optimale en termes de précision, notamment lorsque le nombre de neurones est le plus important. Ainsi, le meilleur modèle à partir de l'ensemble d'entraînement est obtenu pour un nombre de **`r nnet_mod$bestTune$size`** neurones et une pondération de **`r nnet_mod$bestTune$decay`**, donnant une précision de **`r nnet_mod$results %>% filter(size == 5 & decay == 0.1) %>% select(Accuracy)`**. Ici aussi, nous sommes en mesure de visualiser les différents éléments de la matrice de confusion servant à calculer les métriques de qualité du modèle.  

```{r performance_2, message=FALSE, results='hide', comment=NA, warning=FALSE, echo=TRUE}

# Affichage du meilleur modèle 2

nnet_mod$bestTune
nnet_mod$finalModel

# On visualise la performance du modèle 2 

ggplot(data = tibble(resid = nnet_mod$finalModel$residuals),
       aes(x = resid)) +
  geom_histogram() +
  theme_minimal() +
  ggtitle("Nombre de valeurs résiduelles") +
  theme(plot.title = element_text(family = 'Helvetica', face = 'bold', hjust = 0.5)) +
  labs(x = "Valeurs résiduelles", y = "Nombre")

nnet_train_fitted_resid = tibble(fitted = nnet_mod$finalModel$fitted.values,
                                 resid = nnet_mod$finalModel$residuals)

ggplot(data = nnet_train_fitted_resid,
       aes(x = fitted,
           y = resid)) +
  geom_point() +
  theme_minimal() +
  ggtitle("Résidus en fonction des valeurs capturées par le modèle") +
  theme(plot.title = element_text(family = 'Helvetica', face = 'bold', hjust = 0.5)) +
  labs(x = "Valeurs capturées", y = "Résidus")

ggplot(nnet_mod$results, aes(x = size, 
                             y = Accuracy, 
                             color = as.factor(decay))) +
  geom_line() +
  geom_point() +
  scale_color_viridis(discrete = TRUE) +
  theme_minimal() +
  labs(x = "Nombre de neurones", y = "Précision (Accuracy)", color = "Pondération") +
  ggtitle("Apprentissage du réseau de neurones\n - Précision en fonction du nombre de neurones") +
  facet_grid(~decay, labeller = label_both) +
  theme(plot.title = element_text(family = 'Helvetica', face = 'bold', hjust = 0.5))

# Phase de test et matrice de confusion - Modèle 2

dt_pred_class2 = predict(nnet_mod, test_q2)

dt_pred_class2 %>% 
  head(5)

# Matrice de confusion - Modèle 2

dt_conf_mat2 = confusionMatrix(dt_pred_class2,
                               reference = test_q2$Class,
                               positive = "yes",
                               mode = "everything")
```

`r knitr::kable(head(dt_conf_mat2[["table"]], 2), caption = "**Tableau 4 - Matrice de confusion du modèle 2**")`

## Modèle 3 - Analyse discriminante linéaire (lda)

Finalement, pour l'analyse discrimante linéaire, seule la matrice de confusion a été utilisée pour rendre compte de la qualité de la méthode. En outre, le modèle n'acceptant aucun paramètre spécifique en entrée, retourne une précision et un kappa de Cohen uniques de **`r lda_mod$results$Accuracy`** et **`r lda_mod$results$Kappa`** respectivement.

```{r performance_3, message=FALSE, results='hide', comment=NA, warning=FALSE, echo=TRUE}

# Affichage du meilleur modèle 3

lda_mod$finalModel

# Prévision et matrice de confusion - Modèle 3

dt_pred_class3 = predict(lda_mod, test_q2)

dt_pred_class3 %>% 
  head(5)

# Matrice de confusion - Modèle 3

dt_conf_mat3 = confusionMatrix(dt_pred_class3,
                               reference = test_q2$Class,
                               positive = "yes",
                               mode = "everything")

```

`r knitr::kable(head(dt_conf_mat3[["table"]], 2), caption = "**Tableau 5 - Matrice de confusion du modèle 3**")`

# Question 2.5 - Comparaison des modèles

La comparaison des modèles s'est faite à partir de trois méthodes : les prévisions de la probabilité d'appartenance, les courbes ROC et deux facteurs de la matrice de confusion (accuracy et Kappa) représentés sur des boîtes à moustaches.

## Prévisions de la probabilité d'appartenance

De façon générale, on peut visualiser les probabilités d'appartenance de chaque modèle pour voir lequel comporte les probabilités les plus élevées. On peut voir de façon intuitive sur les tableaux 6 à 8, si un modèle semble être plus performant que les autres. Évidemment, nous n'avons accès qu'à un petit échantillon mais il est possible de voir lequel des modèles seraient le plus performant avec une analyse une peu plus large ce qui est difficile dans le cas présent.

```{r predict_mod, message=FALSE, results='hide', comment=NA, warning=FALSE, echo=TRUE}

# Prévisions de la probabilité d'appartenance du modèle 1

dt_pred_prob1 <- predict(rpart2_mod,
                        test_q2,
                        type = "prob")

# Prévisions de la probabilité d'appartenance du modèle 2

dt_pred_prob2 <- predict(nnet_mod,
                        test_q2,
                        type = "prob")

# Prévisions de la probabilité d'appartenance du modèle 3

dt_pred_prob3 <- predict(lda_mod,
                        test_q2,
                        type = "prob")
```

`r knitr::kable(head(dt_pred_prob1, 5), caption = "**Tableau 6 - Prévisions de la probabilité d'appartenance du modèle 1**")`

`r knitr::kable(head(dt_pred_prob2, 5), caption = "**Tableau 7 - Prévisions de la probabilité d'appartenance du modèle 2**")`

`r knitr::kable(head(dt_pred_prob3, 5), caption = "**Tableau 8 - Prévisions de la probabilité d'appartenance du modèle 3**")`

## Courbes ROC

Une des façons les plus efficaces de comparer des modèles est d'utiliser les courbes ROC. On crée cette courbe avec le package pROC qui met en relation le taux de vrais positifs (la sensitivité) et le taux de faux positifs (1 - exactitude).

```{r courbes_roc, message=FALSE, results='hide', comment=NA, warning=FALSE, echo=TRUE}

## Courbe ROC pour le modèle 1

roc_curve1 = roc(response = test_q2$Class,
                predictor = dt_pred_prob1[, "yes"],
                levels = levels(test_q2$Class))

## Courbe ROC pour le modèle 2

roc_curve2 = roc(response = test_q2$Class,
                predictor = dt_pred_prob2[, "yes"],
                levels = levels(test_q2$Class))

## Courbe ROC pour le modèle 3

roc_curve3 = roc(response = test_q2$Class,
                predictor = dt_pred_prob3[, "yes"],
                levels = levels(test_q2$Class))

list(MODELE_1 = roc_curve1, MODELE_2 = roc_curve2, MODELE_3 = roc_curve3) %>%
  ggroc(legacy.axes = TRUE) +
  scale_color_viridis(name = "Modèle", 
                      labels = c("1 - rpart2", "2 - nnet", "3 - lda"), 
                      discrete = TRUE) +
  geom_abline(slope = 1,
              intercept = 0,
              linetype = "dashed") +
  theme_minimal() +
  ggtitle("Courbes ROC des trois modèles") +
  labs(x='Taux de faux positifs (1 - specificity)',
       y='Taux de vrais positifs (sensitivity)',
       title='') +
  annotate('text',
           x=0.5,
           y=0.25, 
           label = paste('AUC =',
                         round(auc(roc_curve1), 2)),
             size = 2) +
    annotate('text',
           x=0.675,
           y=0.25, 
           label = paste('AUC =',
                         round(auc(roc_curve2), 2)),
             size = 2) +
    annotate('text',
           x=0.85,
           y=0.25, 
           label = paste('AUC =',
                         round(auc(roc_curve3), 2)),
             size = 2)
```

Dans le cas des courbes ROC, on utilise l'aire sous chacune des courbes ROC pour évaluer la qualité des modèles. Alors que l'aire sous la courbe du modèle 1 (rpart2) est de **`r auc(roc_curve1)`**, l'aire sous la courbe du modèle 2 (nnet) est de **`r auc(roc_curve2)`** et l'aide sous la courbe du modèle 3 est de **`r auc(roc_curve3)`**. Ainsi, on peut affirmer que le modèle ayant la meilleure performance est le modèle 2 selon cette méthode de comparaison avec un très leger avantage sur le modèle 2.

## Précision et Kappa de Cohen 

Finalement, on peut utiliser les facteurs de précision (accuracy) et le Kappa de Cohen provenant tous deux de la matrice de confusion pour comparer les modèles. La précision mesure le taux de bons résultats (vrais positifs et faux négatifs) sur l'ensemble des résultats tandis que le Kappa de Cohen mesure le degré de concordance entre deux évaluateurs par rapport au hasard ; un kappa proche ou égal à 1 signifiant que la concordance est (quasi) parfaite.

```{r boxplots, message=FALSE, results='hide', comment=NA, warning=FALSE, echo=TRUE}

# On compare la performance des trois modèles en utilisant la fonction resample()
models_compare = resamples(list(RPART2=rpart2_mod, NNET=nnet_mod, LDA=lda_mod))

# Résumé des performances des modèles
summary(models_compare)

# Utilisation de diagrammes en boîte
scales = list(x=list(relation="free"), y=list(relation="free"))
bwplot(models_compare, scales=scales)
```

On peut voir dans les boîtes à moustaches que le modèle 1 (Rpart2) semble le plus performant à partir de l'évaluation de la précision même si ces taux sont très proches l'un de l'autre (autour de 0.81). Par contre, le Kappa de Cohen semble démontrer que le modèle 2 (lda) est le modèle le plus performant dans une plus grande mesure.
