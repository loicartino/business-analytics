---
title: "Travail pratique 2"
subtitle: "<span style='font-size: 35px'>Question 1 -  Estimation de l’efficacité d’une opération de recherche en mer</style>" 
author:
- Équipe 17
- Loïc Artino (536 756 361)
- François L'Écuyer (901 346 493)
date: "28 octobre 2020"
output:
  html_document:
    code_folding: "hide"
    highlight: haddock
    output: html_document
    theme: default
    toc : true
    toc_float: true
    toc_depth: 2
---
```{r librairies, include=FALSE}
packages<-function(x){
  x<-as.character(match.call()[[2]])
  if (!require(x,character.only=TRUE)){
    install.packages(pkgs=x,repos="http://cran.r-project.org")
    require(x,character.only=TRUE)
  }
}
packages(tidyverse)
packages(rmarkdown)
packages(caret)
packages(viridis)
packages(gbm)
packages(corrplot)
packages(caret)
packages(egg)

# corrplot est un outil permettant de facilement visualiser les matrices de corrélation
# viridis est un outil de gestion des couleurs dans les visules de type ggplot
# la fonction ggarrange() d'egg est un outil permettant de combiner plusieurs objets de type ggplot

```

# Question 1.1 - Préparer les données

Avant d'effectuer l'entraînement, il importe de préparer convenablement les données. Dans le cadre de ce travail, la préparation s'est faite selon quatre étapes. La première a consisté à télécharger les données à partir d'un fichier CSV alors que la deuxième étape a consisté à vérifier le format des colonnes et si le fichier contenait des valeurs manquantes. La troisième étape a consisté à supprimer les variables qui étaient fortement corrélées avec d'autres variables tandis que la dernière étape a consisté à partitionner les données en deux jeux de données d'entraînement et de test de façon pseudo-aléatoire.  

## Téléchargement des données

```{r download, message=FALSE, results='hide', comment=NA, warning=FALSE, echo=TRUE}

# Téléchargement du fichier CSV (Champs séparés par des virgules)

data_q1 = read_csv("data/Q1_data.txt")

```
La première étape de la préparation est le téléchargement des données qui a permis de recevoir **`r nrow(data_q1)`** observations réparties dans **`r ncol(data_q1)`** colonnes dont la dernière est la variable dépendante. On peut voir une partie de ces données dans le Tableau 1.

`r knitr::kable(head(data_q1, 5), caption = "**Tableau 1 - Liste partielle des données**")`

## Vérification du format des colonnes et des valeurs manquantes

```{r manquantes, message=FALSE, results='hide', comment=NA, warning=FALSE, echo=TRUE}

# Vérification du format des colonnes

column_types <- sapply(data_q1, class)

# Élimination des observations avec des valeurs manquantes

data_q1 %>%
  filter_all(any_vars(is.na(.)))

data_q1 = data_q1 %>% drop_na()

# Compilation des valeurs manquantes éventuelles

data_q1 %>%
  summarise(na_count = sum(is.na(.)))

```

La deuxième étape de la préparation des données a consisté à vérifier si le format des colonnes était approprié et à supprimer les enregistrements comportant des données manquantes. Pour ce faire, nous avons utilisé la  fonction **sapply**, dont on peut voir la résultante au Tableau 2 et ainsi confirmer que le format de toutes les colonnes étaient numérique (dbl), ce qui est approprié pour les analyses qui vont suivre.

`r knitr::kable(head(column_types, 23), caption = "**Tableau 2 - Type des colonnes**")`

De plus, pour s'assurer de l'intégrité de toutes les valeurs des **`r nrow(data_q1)`** observations, une vérification et une élimination des observations avec des valeurs manquantes a été faites. Cette opération nous a permis de constater qu'il y avait **`r sum(is.na(data_q1))`** valeur(s) manquante(s) dans notre table de données. 

## Élimination des variables corrélées

La troisième étape de préparation des données a consisté à vérifier si certaines données étaient corrélées entre elles. Cela permet d'améliorer la validité des résultats et de réduire le nombre de variables nécessaires aux calculs. Cela a été fait en créant une matrice de corrélation que nous avons affichée sous forme d'un corrélogramme permettant de visualiser le niveau de corrélation entre chaque variable.

```{r correlation, message=FALSE, results='hide', comment=NA, warning=FALSE, echo=TRUE, fig.align='center'}

# Création d'une matrice de corrélation

mat_1 <- round(cor(data_q1[,1:22]),2)

# Affichage de la matrice sous forme d'un corrélogramme

corrplot(mat_1, method = "circle")

```

On peut voir sur le corrélogramme que plusieurs variables ont des corrélations importantes avec d'autres variables, ce qui nous permet d'éliminer quelques unes. Cela a été fait à l'aide de la fonction **findCorrelation** où le seuil minimal de corrélation a été fixée à 0.75.

```{r select, message=FALSE, results='hide', comment=NA, warning=FALSE, echo=TRUE}

# Création d'une table sans la variables dépendantes

data_ind <- data_q1 %>% select(-outcome)

# Rechercher les corrélations supérieures au cutoff

too_high <- findCorrelation(cor(data_ind), cutoff = .75, verbose = TRUE)

# Suppression des colonnes ayant une corrélation supérieure au cutoff et ajout de la variable dépendante

datanoncorr <- data_q1 %>% select(-too_high, outcome)

```

Suite à la suppression des variables fortement corrélées, on constate qu'il ne reste que six variables indépendantes en plus de la variables dépendantes  (outcome), dont on peut voir une partie au Tableau 3.

`r knitr::kable(head(datanoncorr, 5), caption = "**Tableau 3 - Varialbes utilisées pour l'analyse**")`

N.B. Les postulats nécessaires à la création et l'utilisation d'une matrice de corrélation sur l'ensemble des variables comme le test de normalité n'ont pas été vérifiés dans le cadre de ce travail.

## Partionnement des ensembles

```{r partition, message=FALSE, results='hide', comment=NA, warning=FALSE, echo=TRUE}

# Partitionnement des données en ensembles d'entrainement et de test

set.seed(536)

# Ici, p = 0.75 signifie que 75% des données sont utilisées pour l'ensemble d'entrainement
# times = 1 signifie qu'on répète le processus 1 fois

train_q1_indices = createDataPartition(y = data_q1$outcome,
                                       p = 0.75,
                                       list = FALSE,
                                       times = 1)

train_q1 = datanoncorr %>% slice(train_q1_indices)
test_q1 = datanoncorr %>% slice(-train_q1_indices)

```

Finalement, la dernière étape consiste à partitionner les observations en deux ensembles de façon pseudo-aléatoire. Un ensemble d'entraînement permettra de construire le meilleur modèle et un ensemble test permettra de vérifier la performance de ce modèle. La commande **set.seed()** a été fixée à **536** de façon arbitraire pour pouvoir partitionner les données de façon aléatoire mais selon un même schème pour pouvoir comparer nos résultats d'une fois à l'autre. Les paramètres de la partition ont été fixés à 0.75 pour le **p** de façon à avoir 70 % de données dans la partition d'entraînement et à 1 pour le paramètres **times** de façon à avoir seulement une partition pour les deux ensembles d'entraînement et de test. Cette création a permis d'obtenir **`r nrow(train_q1)`** observations pour l'ensemble d'entraînement et **`r nrow(test_q1)`** pour l'ensemble test. 

# Question 1.2 - Entraînement

Nous avons décidé de comparer trois méthodes pour estimer l'efficacité d'une opération de recherche en haute mer. La première est le k-nearest neighbors (knn), le second est le generalized linear model (glm) et le troisième est le gradient boosted machine (gbm).

La première étape de l'entrainement a consisté à créer un objet trainControl qui permet d'en spécifier les paramètres. À cet effet, nous avons utiliser la méthode **repeatedcv** qui nous permet de définir le nombre de partition avec lesquels nous désirons entraîner nos données et le nombre de répétition que nous désirons faire. Dans ce cas, le nombre de partitions a été fixée à **10** et le nombre de répétitions à **10** aussi. La deuxième étape a consisté à produire des modèles différents à partir de ce jeu de données pour sélectionner ceux qui convenaient le mieux pour chaque méthode selon des paramètres qui leurs sont propres, et cela, à partir d'une fichier RDS créé lors de la première utilisation de chaque méthode.

## Modèle 1 - k-nearest neighbors (knn) 

Le modèle k-nearet neighbors (méthode des k plus proches voisins en français) est une méthode d'apprentissage supervisé qui consiste à utiliser la moyenne pondérée d'un nombre k des valeurs les plus proches et calcule la distance euclidienne entre la variable d'entrée et celle à tester. La racine de l'erreur quadratique moyenne, ainsi que l'écart quadratique moyen (R-carré) sont utilisés comme critères afin de sélectionner la valeur de k optimale. 

```{r mod_1, echo=TRUE, message=FALSE, warning=FALSE, comment=NA, results='hide'}

### Phase d'entrainement - Modèle 1 - K-nearest neighbours

# Création de la structure de données avec la fonction cross-validation). Sera utilisée pour tous les modèles.

dt_fit_control = trainControl(method = "repeatedcv",
                                number = 10,
                                repeats = 10)
if (file.exists("knn_model.rds") == TRUE) {
  knn_model = readRDS("knn_model.rds")
} else {

  knn_tune = expand.grid(k = 1:25)
  
  knn_model = train(outcome ~ .,
                    data = train_q1,
                    trControl = dt_fit_control,
                    method = "knn",
                    preProcess = c("center", "scale"),
                    tuneGrid = knn_tune)
  saveRDS(knn_model, file = "knn_model.rds")

}

knn_model

```

## Modèle 2 - Generalized linear model (glm)

La méthode du generalized linear model (régression linéaire généralisé en français) est une méthode basée sur une régression linéaire simple visant à minimiser l'écart entre des données et l'estimation linéaire d'une fonction souvent à partir de la méthode des moindres carrés.

```{r mod_2, message=FALSE, results='hide', comment=NA, warning=FALSE, echo=TRUE}

### Modèle 2 - Generalized Linear Model

if (file.exists("glm_model.rds") == TRUE) {
  glm_model = readRDS("glm_model.rds")
} else {

# Generalized Linear Models ne nécessite pas de paramètre en entrée
  
  glm_model = train(outcome ~ .,
                   data = train_q1,
                   trControl = dt_fit_control,
                   preProcess = c("center", "scale"),
                   method = "glm")
  saveRDS(glm_model, file = "glm_model.rds")
}

glm_model

```

## Modèle 3 - Gradient boosted machine (gbm)

La méthode du gradiant boosting machine est une technique d'apprentissage produisant un modèle de prédiction à partir d'un algorithme d'optimisation par arbres de décision. Tout d'abord, la profondeur d'interaction (interaction.depth) décrit le nombre de possibilités (branches) partant de chaque nœud d'un arbre de décision. Ainsi, nous avons fixé ces valeurs possibles à 1, 3, 5 et 9 branches. Le nombre d'arbres (n.trees) correspond au nombre d'arbres à produire, soit le nombre d'itérations à réaliser. Il est ici de 1200. Cependant, il est intéressant de noter que choisir un trop grand nombre peut mener à du surapprentissage.Ensuite, le taux d'apprentissage (shrinkage) est une valeur qui permet de réguler l'étape d'apprentissage du modèle GBM et ainsi apporter un certain contrôle sur de potentielles erreurs lors des itérations. Finalement, le nombre d'observations minimale à chaque noeud (n.minobsinnode) correspond au nombre de valeurs retrouvées aux nœuds terminaux des arbres. Nous l'avons ici fixé à vingt observations.

```{r mod_gbm, message=FALSE, results='hide', comment=NA, warning=FALSE, echo=TRUE}

if (file.exists("gbm_model.rds") == TRUE) {
  gbm_model = readRDS("gbm_model.rds")
} else {
    
  tune_gbm = expand.grid(interaction.depth = c(1, 3, 5, 9), 
                        n.trees = (1:40)*30, 
                        shrinkage = 0.1,
                        n.minobsinnode = 20)
  
  gbm_model = train(outcome ~ .,
                   data = train_q1,
                   trControl = dt_fit_control,
                   preProcess = c("center", "scale"),
                   method = "gbm",
                 tuneGrid = tune_gbm)
  saveRDS(gbm_model, file = "gbm_model.rds")
}

```

# Question 1.3 - Évaluation de l’entraînement pour chaque modèle individuel

En ce qui concerne l'entraînement des modèles, nous avons choisi d'entraîner 75% des données, validée par la méthode croisée et répétée pour nos trois modèles. Ainsi, les données d'entraînement ont été aléatoirement distribuées et divisées en dix sous-partitions (folds), chacune testée à l'entraînement contre les k-1 (neuf dans notre cas) autres sous-partitions, et ce à chaque itération, afin d'obtenir le meilleur modèle possible. De plus, l'ensemble d'entraînement a été prétraité de la même manière, à savoir normalisé et mis sur une échelle comparable.

## Évaluation des résidus du modèle 1 (knn) 

```{r mod_1res, message=FALSE, results='hide', comment=NA, warning=FALSE, echo=TRUE, fig.align='center'}

# Prévisions sur l'ensemble d'entrainement

pred1 = predict(knn_model, train_q1)
pred2 = predict(glm_model, train_q1)
pred3 = predict(gbm_model, train_q1)

## Performance du modèle 1 (knn) 

knn_model$bestTune

knn_a = knn_model$results %>%
  mutate(best_k = ifelse(k == 7, TRUE, FALSE)) %>%
  ggplot(aes(x = k,
           y = RMSE)) +
  geom_ribbon(aes(ymin = RMSE - RMSESD,
              ymax = RMSE + RMSESD), alpha = 0.2) +
  geom_point(aes(color = best_k)) +
  geom_line() +
  scale_color_manual(values = c('#595959', 'red')) +
  theme_minimal() +
  ggtitle("Racine de l'erreur quadratique moyenne \nen fonction du nombre de plus proches voisins") +
  theme(plot.title = element_text(family = 'Helvetica', face = 'bold', hjust = 0.5, size = 10),
        axis.title.x = element_text(size = 8),
        axis.title.y = element_text(size = 8),
        legend.position = "none")

knn_b = ggplot(data = tibble(resid = train_q1$outcome - pred1), aes(x = resid)) +
  geom_histogram() +
  theme_minimal() +
  ggtitle("Nombre de valeurs résiduelles") +
  theme(plot.title = element_text(family = 'Helvetica', face = 'bold', hjust = 0.5, size = 10),
        axis.title.x = element_text(size = 8),
        axis.title.y = element_text(size = 8)) +
  labs(x = "Valeurs résiduelles", y = "Nombre")

ggarrange(knn_a, knn_b, ncol = 2)

```

Voici les résultats du modèle KNN : 

`r knitr::kable(knn_model$bestTune, caption = "**Modèle 1 - Paramètres optimaux**")`

D'après le modèle, une valeur de k égale à sept est celle qui produit la meilleure performance. En effet, le graphique ci-dessus atteste de ces résultats. Le meilleur modèle y est représenté par le point rouge, nous remarquons que celui-ci possède la plus faible RMSE parmi toutes les valeurs de k testées, soit **`r knn_model$results %>% filter(k == 7) %>% select(RMSE)`**. D'autre part, les résidus sont centrés et bien distribués autour de zéro, ce qui est bon signe.

## Évaluation des résidus du modèle 2 (glm)

```{r mod_2_res, message=FALSE, results='hide', comment=NA, warning=FALSE, echo=TRUE, fig.align='center'}

glm_a = ggplot(data = tibble(resid = glm_model$finalModel$residuals), aes(x = resid)) +
  geom_histogram() +
  theme_minimal() +
  ggtitle("Nombre de valeurs résiduelles") +
  theme(plot.title = element_text(family = 'Helvetica', face = 'bold', hjust = 0.5, size = 10),
        axis.title.x = element_text(size = 8),
        axis.title.y = element_text(size = 8)) +
  labs(x = "Valeurs résiduelles", y = "Nombre")

glm_fitted_resid = tibble(fitted = glm_model$finalModel$fitted.values, resid = glm_model$finalModel$residuals)

glm_b = ggplot(data = glm_fitted_resid, 
       aes(x = fitted,
           y = resid)) + 
  geom_point() +
  theme_minimal() +
  ggtitle("Résidus en fonction des valeurs \ncapturées par le modèle") +
  theme(plot.title = element_text(family = 'Helvetica', face = 'bold', hjust = 0.5, size = 10),
        axis.title.x = element_text(size = 8),
        axis.title.y = element_text(size = 8)) +
  labs(x = "Valeurs capturées", y = "Résidus")

glm_train_fitted_real = tibble(real_y = train_q1$outcome,
pred_y = glm_model$finalModel$fitted.values)

ggarrange(glm_a, glm_b, ncol = 2, nrow = 1)

```

En ce qui concerne le second modèle entraîné, celui-ci présente des résidus sensiblement centrés. Néanmoins, on remarque une asymétrie négative (vers la gauche) des résidus, en plus de leur nombre relativement élevés. Si on observe le comportement des résidus par rapport aux valeurs correctement capturées par le modèle, on se rend compte que celles-ci ne sont pas distribuées selon une asymptote, nous pouvons en déduire que la relation entre les variables ne s'opère pas de façon linéaire, ce qui pourrait expliquer l'aspect parabolique du nuage de points. Enfin, le RMSE étant notre critère d'évaluation principal, vient appuyer cette hypothèse, avec une valeur de **`r glm_model$results$RMSE`** et un R-carré de **`r glm_model$results$Rsquared`**.

## Évaluation des résidus du modèle 3 (gbm)

Pour le troisième modèle, les différents paramètres mentionnés pour celui-ci doivent être pris en compte simultanément dans l'évaluation de la performance du modèle. Ainsi, la meilleure combinaison semble être la suivante : 

`r knitr::kable(gbm_model$bestTune)`

```{r mod_3res, message=FALSE, results='hide', comment=NA, warning=FALSE, echo=TRUE, fig.align='center'}

gbm_model$bestTune

gbm_model$results %>% head(5)

gbm_a = gbm_model$results %>%
  mutate(best_tune = ifelse(n.trees == 60 & interaction.depth == 9 & shrinkage == 0.1 & n.minobsinnode == 20, TRUE, FALSE)) %>%
  ggplot(aes(x = n.trees,
            y = RMSE, col = as.factor(interaction.depth))) +
    geom_point(aes(shape = best_tune), show.legend = FALSE) +
    geom_line() +
    theme_minimal() +
    scale_color_viridis(discrete = TRUE, direction = -1) +
    scale_shape_manual(values = c(19, 5)) +
    labs(x = "\n Nombre d'itérations", col = "Profondeur \nd'interaction") +
    ggtitle("Racine de l'erreur quadratique moyenne obtenue \nen fonction du nombre d'itérations") +
    theme(plot.title = element_text(family = 'Helvetica', face = 'bold', hjust = 0.5, size = 10),
          axis.title.x = element_text(size = 8),
          axis.title.y = element_text(size = 8),
          legend.text = element_text(size = 8))

gbm_b = ggplot(data = tibble(resid = train_q1$outcome - pred3), aes(x = resid)) +
  geom_histogram() +
  theme_minimal() +
  ggtitle("Nombre de valeurs résiduelles") +
  theme(plot.title = element_text(family = 'Helvetica', face = 'bold', hjust = 0.5, size = 10),
        axis.title.x = element_text(size = 8),
        axis.title.y = element_text(size = 8)) +
  labs(x = "Valeurs résiduelles", y = "Nombre")

ggarrange(gbm_a, gbm_b, ncol = 2)

```

Le graphique ci-dessus montre une performance globale du modèle. Nous remarquons tout d'abord qu'il est important d'avoir une profondeur d'interaction supérieure à 1 afin de garantir un RMSE moindre (le cas contraire signifiant qu'il n'y aurait qu'une seule possibilité à chaque noeud dans un arbre donné) d'où l'importance de réaliser un certain nombre d'itérations. Néanmoins, on observe que l'erreur quadratique moyenne a tendance à augmenter à chaque itération subséquente, d'où le risque d'un surapprentissage du modèle. En ce qui concerne les résidus, ces derniers sont centrés et bien distribués autour de zéro, de sorte que le modèle surestime ou sous estime parfois la valeur à prédire. Enfin, le meilleur modèle est représenté par un losange vide, soit celui qui possède un RMSE minimal, de **`r gbm_model$results %>% filter(n.trees == 60 & interaction.depth == 9 & shrinkage == 0.1 & n.minobsinnode == 20) %>% select(RMSE)`**.

# Question 1.4 - Comparaison des modèles

## Prédictions vs valeurs réelles

Lorsqu'on observe les prédictions du modèle KNN en fonction des valeurs réelles ci-dessous, on constate que les données se distribuent de manière assez régulière à ce qui serait attendu (droite en bleu), malgré quelques dispersions sur des valeurs inférieures.

```{r message=FALSE, results='hide', comment=NA, warning=FALSE, echo=TRUE, fig.align='center'}

# Phase de test des modèles

dt_pred_class1 = predict(knn_model, test_q1)

dt_pred_class2 = predict(glm_model, test_q1)

dt_pred_class3 = predict(gbm_model, test_q1)

# Modèle 1

knn_train_fitted_real = tibble(real_y = test_q1$outcome,
                               pred_y = dt_pred_class1)

ggplot(data = knn_train_fitted_real, aes(x = real_y,
                                        y = pred_y)) + 
  geom_point() +
  geom_point(aes(x = real_y, 
                 y = real_y),
                 color = 'blue',
                 shape = 15) +
  coord_fixed() +
  theme_minimal() +
  labs(x = "Valeurs réelles", y = "Valeurs prédites") +
  ggtitle("Modèle KNN - Valeurs prédites par rapport aux valeurs réelles") +
  theme(plot.title = element_text(family = 'Helvetica', face = 'bold', hjust = 0.5))
```

En revanche, tel que constaté dans pour l'ensemble d'entraînement précédemment, le modèle linéaire (glm) ne permet pas d'expliquer et prédire de façon adéquate la variable de sortie. En effet, les valeurs prédites par le modèle et les valeurs réelles de l'ensemble de test ne suivent que très peu la droite en bleu, matérialisant le modèle optimal attendu.

```{r message=FALSE, results='hide', comment=NA, warning=FALSE, echo=TRUE, fig.align='center'}
# Modèle 2

glm_train_fitted_real = tibble(real_y = test_q1$outcome,
                               pred_y = dt_pred_class2)
ggplot(data = glm_train_fitted_real, 
       aes(x = real_y,
           y = pred_y)) + 
  geom_point() +
  geom_point(aes(x = real_y, y = real_y),
             color = 'blue',
             shape=15) + 
  coord_fixed() +
  theme_minimal() +
  labs(x = "Valeurs réelles", y = "Valeurs prédites") +
  ggtitle("Modèle GLM - Valeurs prédites par rapport aux valeurs réelles") +
  theme(plot.title = element_text(family = 'Helvetica', face = 'bold', hjust = 0.5))
```

Enfin, le modèle GBM semble le plus prometteur, puisque les données sont très centrées sur le modèle optimal, ce qui démontre un fort potentiel de prédiction sur l'ensemble de test. On peut voir que les valeurs réelles suivent de façon très nettes la courbe prédicte par le modèle.

```{r message=FALSE, results='hide', comment=NA, warning=FALSE, echo=TRUE, fig.align='center'}
# Modèle 3

gbm_train_fitted_real = tibble(real_y = test_q1$outcome,
                               pred_y = dt_pred_class3)

ggplot(data = gbm_train_fitted_real, aes(x = real_y,
                                        y = pred_y)) + 
  geom_point() +
  geom_point(aes(x = real_y, 
                 y = real_y),
                 color = 'blue',
                 shape = 15) +
  coord_fixed() +
  theme_minimal() +
  labs(x = "Valeurs réelles", y = "Valeurs prédites") +
  ggtitle("Modèle GBM - Valeurs prédites par rapport aux valeurs réelles") +
  theme(plot.title = element_text(family = 'Helvetica', face = 'bold', hjust = 0.5))

```

## Comparaison finale entre les modèles

La comparaison finale entre les modèles s'est faite en utilisant les valeurs du MAE (mean absolute error), du RMSE (root mean squared error) et le Rsquared. On peut voir sur la figure suivante ces différentes métriques qui sont illustrées sur des boîtes à moustaches. 

```{r best_model, message=FALSE, results='hide', comment=NA, warning=FALSE, echo=TRUE, fig.align='center'}

# Prédictions

pred = tibble(KNN = dt_pred_class1, GLM = dt_pred_class2, GBM = dt_pred_class3)

pred %>%
  head(5)

# Prédictions des modèles pour l'ensemble de test (idem pour le tableau)

tibble(Metrics = c("RMSE", "Rsquared", "MAE"),
       KNN = postResample(pred = dt_pred_class1, obs = test_q1$outcome),
       GLM = postResample(pred = dt_pred_class2, obs = test_q1$outcome),
       GBM = postResample(pred = dt_pred_class3, obs = test_q1$outcome)
)

# On compare la performance des trois modèles en utilisant la fonction resample()
model_list = resamples(list(KNN=knn_model, GLM=glm_model, GBM=gbm_model))

# Résumé des performances des modèles
summary(model_list)

# Utilisation de diagrammes en boîte
scales = list(x=list(relation="free"), y=list(relation="free"))
bwplot(model_list, scales=scales)

# On enregistre ici le meilleur modèle dans un fichier RDS séparé

if (file.exists("best_model.rds") == TRUE) {
  best_model = readRDS("best_model.rds")
} else {
  
# On compare et calcule les différents RMSE moyennes pour chaque modèle
  
  rmse = model_list$values %>%
    rename(., KNN = `KNN~RMSE`, GLM = `GLM~RMSE`, GBM = `GBM~RMSE`) %>%
    select(KNN, GLM, GBM) %>%
    colMeans()
  
# RMSE minimum
  
  min_rmse = min(rmse)
  
# On sélectionne le meilleur modèle

  if(rmse[1] == min_rmse) {
    best_model = knn_model
  } else if (rmse[2] == min_rmse) {
    best_model = glm_model
  } else {
    best_model = gbm_model
  }

# On sauve en format RDS le meilleur modèle (celui avec le meilleur RMSE)  

  saveRDS(best_model, "best_model.rds") 
}  

```

On peut voir sur le graphique précédent que la modèle 3 (gbm) est clairement le plus performant. Non seulement, il minimise les erreurs (MAE et RMSE) entre les valeurs prédites et les valeurs réelles mais il a aussi une bien meilleure prédictibilité selon le calcul du R carré (Rsquared). Nous avons donc sélectionné celui-ci comme étant le meilleur modèle.
